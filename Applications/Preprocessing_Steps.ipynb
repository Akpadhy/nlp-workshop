{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Steps\n",
    "\n",
    "Most of the NLP datasets have a lot of inconsistencies that cannot be directly used.\n",
    "\n",
    "### Removing non alpha numeric (special) characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_nonalphanumeric(text):\n",
    "    text = re.sub(\"[^a-zA-Z0-9 ]\", \" \", text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_lower(text):\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "        E.g., This is a sentence -> [This, is, a, sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenization(text):\n",
    "    tokens = text.split()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "        E.g., Removing stopword list, i.e., This is a sentence -> This sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "english_stopwords = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(tokens):\n",
    "    non_stopwords = []\n",
    "    for t in tokens:\n",
    "        if t not in english_stopwords:\n",
    "            non_stopwords.append(t)\n",
    "    return non_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "       - Stemming: Converting a word into its base form using heuristic rules like removing \"ing\", \"s\", etc.\n",
    "       - E.g., runs -> run, walking -> walk\n",
    "       - Lemmatization: Converting a word into its base form using morphological analysis\n",
    "       - E.g., ran -> run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def stemmer(tokens):\n",
    "    lemmas = []\n",
    "    for token in tokens:\n",
    "        lemmas.append(snowball_stemmer.stem(token))\n",
    "    \n",
    "    return lemmas\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run', 'walk']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer(['runs', 'walking'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "\n",
    "def lemmatizer(tokens, pos_tags):\n",
    "    lemmas = []\n",
    "    for i in range(len(tokens)):\n",
    "        token = tokens[i]\n",
    "        pos_tag = pos_tags[i]\n",
    "        wordnet_tag = get_wordnet_pos(pos_tag)\n",
    "        lemmas.append(wordnet_lemmatizer.lemmatize(token, wordnet_tag))\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer(['ran'], ['VB'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is WordNet?\n",
    "\n",
    "WordNet is a freely distributed English lexical database. It loosely resembles a thesaurus where words are organized with their synonyms and antonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('bank.n.01'),\n",
       " Synset('depository_financial_institution.n.01'),\n",
       " Synset('bank.n.03'),\n",
       " Synset('bank.n.04'),\n",
       " Synset('bank.n.05'),\n",
       " Synset('bank.n.06'),\n",
       " Synset('bank.n.07'),\n",
       " Synset('savings_bank.n.02'),\n",
       " Synset('bank.n.09'),\n",
       " Synset('bank.n.10'),\n",
       " Synset('bank.v.01'),\n",
       " Synset('bank.v.02'),\n",
       " Synset('bank.v.03'),\n",
       " Synset('bank.v.04'),\n",
       " Synset('bank.v.05'),\n",
       " Synset('deposit.v.02'),\n",
       " Synset('bank.v.07'),\n",
       " Synset('trust.v.01')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets(\"bank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the funds held by a gambling house or the dealer in some gambling games'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets(\"bank\")[5].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset('bank.n.05')\n",
    "w2 = wordnet.synset('bank.n.06')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spell Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import spell\n",
    "def get_spell_corrections(tokens):\n",
    "    n_tokens = []\n",
    "    for token in tokens:\n",
    "        n_tokens.append(spell(token))\n",
    "    return n_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Steps combined together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"ThES is a very*hard to READ sentance.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hard', 'read', 'sentenc']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer(remove_stopwords(get_spell_corrections(tokenization(convert_lower(remove_nonalphanumeric(text))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-Of-Speech Tagging\n",
    "\n",
    "Identifying the Part of Speech tags for each word of a sentence. These tags help us in identifying the grammar structure of a sentence along with its intents and objects. E.g., Noun, Verb, Adjective, etc.\n",
    "\n",
    "The complete set of POS tags are derived from PENN Treebank tags. Ref: [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pos_tags(sent):\n",
    "    pos_tags = nltk.pos_tag(nltk.word_tokenize(sent))\n",
    "    return pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ram', 'NNP'), ('killed', 'VBD'), ('Ravan', 'NNP')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pos_tags(\"Ram killed Ravan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noun Phrase Extraction / Chunking\n",
    "A word or group of words containing a noun and functioning in a sentence as subject, object, or prepositional object.\n",
    "    - [We] are attending [NLP Classes].\n",
    "    - [Ram] killed [Ravan].\n",
    "\n",
    "This is a very important step in a NLP pipeline as this step is often used to extract insights from NLP datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_noun_phrases(pos_tags):\n",
    "    grammar = r\"\"\"\n",
    "              NP: {<PP\\$>?<JJ>*<NN>+} \n",
    "                  {<NN>*<NNP>+}                # chunk sequences of proper nouns\n",
    "                  {<NN>*<NNS>+}\n",
    "                  {<NN>+}\n",
    "            \"\"\"\n",
    "        #{<DT|PP\\$>?<JJ>*<NN>}   # chunk determiner/possessive, adjectives and noun\n",
    "    phrases = []\n",
    "    chunkParser = nltk.RegexpParser(grammar)\n",
    "    chunked = chunkParser.parse(pos_tags)\n",
    "    for subtree in chunked.subtrees(filter=lambda t: t.label() == 'NP'):\n",
    "        phrase = \" \".join([np[0] for np in subtree.leaves()])\n",
    "        phrases.append(phrase)\n",
    "    return phrases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jim', 'shares', 'Acme Corp.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_noun_phrases(get_pos_tags(\"Jim bought 300 shares of Acme Corp. in 2006.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "\n",
    "It is the task of identifying and extracting entities of type PERSON, ORGANIZATION, LOCATION, TIME, MONETARY VALUE, etc.\n",
    "\n",
    "    - E.g., [Jim/PERSON] bought 300 shares of [Acme Corp./ORGANIZATION] in [2006/TIME]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from nltk.chunk import tree2conlltags\n",
    "def get_ner_tags(pos_tags):\n",
    "    ner_chunks = nltk.ne_chunk(pos_tags)\n",
    "    ner_tagged = tree2conlltags(ner_chunks)\n",
    "    ner_type = ''\n",
    "    ner_str = ''\n",
    "    ner_dict = defaultdict(lambda: [])\n",
    "    \n",
    "    for tags in ner_tagged:\n",
    "        tag = tags[2]\n",
    "        if tag == 'O':\n",
    "            if len(ner_str) > 0:\n",
    "                ner_dict[ner_type].append(ner_str)\n",
    "            ner_str = ''\n",
    "            ner_type = ''\n",
    "            continue\n",
    "        \n",
    "        if tag[:2] == \"B-\":\n",
    "            if len(ner_str) > 0:\n",
    "                ner_dict[ner_type].append(ner_str)\n",
    "            \n",
    "            ner_str = tags[0]\n",
    "            ner_type = tag[2:]\n",
    "        \n",
    "        if tag[:2] == 'I-':\n",
    "            ner_str += \" \" + tags[0]\n",
    "        \n",
    "    \n",
    "    if len(ner_str) > 0:\n",
    "        ner_dict[ner_type].append(ner_str)\n",
    "    \n",
    "    return ner_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.get_ner_tags.<locals>.<lambda>>,\n",
       "            {'ORGANIZATION': ['Acme Corp.'], 'PERSON': ['Jim']})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ner_tags(get_pos_tags('Jim bought 300 shares of Acme Corp. in 2006.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Readings\n",
    "1. http://norvig.com/spell-correct.html\n",
    "2. https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "3. https://wordnet.princeton.edu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
