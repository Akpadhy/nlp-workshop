{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Engine (Information Retrieval)\n",
    "\n",
    "### What?\n",
    "\n",
    "- To find documents which would relevant information that can satisfy an information need (i.e., Search Query)\n",
    "- E.g., Google Search\n",
    "\n",
    "\n",
    "### Why?\n",
    "\n",
    "    - Duh.\n",
    "\n",
    "\n",
    "### How?\n",
    "\n",
    "### Fetch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_data():\n",
    "    import glob\n",
    "    abstract_files = glob.glob(\"../data/Hulth2003/train/*.abstr\")\n",
    "    full_data = []\n",
    "    for file in abstract_files:\n",
    "        f = open(file, 'rb')\n",
    "        lines = f.readlines()\n",
    "        file_data = \" \".join([str(line.decode(\"utf-8\").strip()) for line in lines])\n",
    "        full_data.append(file_data)\n",
    "    return full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_data = fetch_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "1. Remove non alpha numeric\n",
    "2. Remove stop words\n",
    "3. Convert Lower\n",
    "4. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "english_stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def get_preprocessing(text):\n",
    "    def remove_nonalphanumeric(text):\n",
    "        text = re.sub(\"[^a-zA-Z0-9 ]\", \" \", text)\n",
    "        text = text.strip()\n",
    "        return text\n",
    "\n",
    "    def convert_lower(text):\n",
    "        text = text.lower()\n",
    "        return text\n",
    "\n",
    "    def remove_stopwords(tokens):\n",
    "        non_stopwords = []\n",
    "        for t in tokens:\n",
    "            if t not in english_stopwords:\n",
    "                non_stopwords.append(t)\n",
    "        return non_stopwords\n",
    "    \n",
    "    def tokenization(text):\n",
    "        tokens = text.split()\n",
    "        return tokens\n",
    "    \n",
    "    preprocessed = remove_stopwords(tokenization(convert_lower(remove_nonalphanumeric(text))))\n",
    "    return preprocessed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['separate',\n",
       " 'accounts',\n",
       " 'go',\n",
       " 'mainstream',\n",
       " 'investment',\n",
       " 'new',\n",
       " 'entrants',\n",
       " 'shaking',\n",
       " 'separate',\n",
       " 'account',\n",
       " 'industry',\n",
       " 'supplying',\n",
       " 'web',\n",
       " 'based',\n",
       " 'platforms',\n",
       " 'give',\n",
       " 'advisers',\n",
       " 'tools',\n",
       " 'pick',\n",
       " 'independent',\n",
       " 'money',\n",
       " 'managers']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_preprocessing(full_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Inverse Document Frequency of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "def get_inverse_document_frequency(candidates):\n",
    "    \n",
    "    total_docs = len(candidates)\n",
    "    doc_unique_phrases = []\n",
    "    for datum in candidates:\n",
    "        unique_phrases = list(set(datum))\n",
    "        doc_unique_phrases.extend(unique_phrases)\n",
    "    \n",
    "    doc_freq = Counter(doc_unique_phrases)\n",
    "    \n",
    "    full_unique_phrases = doc_freq.keys()\n",
    "    inv_doc_freq = defaultdict()\n",
    "    for phrase in full_unique_phrases:\n",
    "        inv_doc_freq[phrase] = math.log(total_docs / (1.0 + doc_freq[phrase]))\n",
    "    \n",
    "    return inv_doc_freq\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_data = [get_preprocessing(datum) for datum in full_data]\n",
    "inverse_doc_freq = get_inverse_document_frequency(full_data)\n",
    "\n",
    "def get_inverted_index(full_data, inverse_doc_freq):\n",
    "    inverted_index = defaultdict(lambda : defaultdict(lambda : 0))\n",
    "    for doc_id in range(len(full_data)):\n",
    "        doc = full_data[doc_id]\n",
    "        term_freq = Counter(doc)\n",
    "        doc_len = len(doc)\n",
    "\n",
    "        terms = term_freq.keys()\n",
    "        for term in terms:\n",
    "            tf = (term_freq[term]/(1.0 * doc_len)) \n",
    "            idf = inverse_doc_freq[term]\n",
    "            \n",
    "            inverted_index[term][doc_id] = tf*idf\n",
    "    \n",
    "    return inverted_index\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inverted_index = get_inverted_index(full_data, inverse_doc_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = \"atomism thesis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_prepcsd = get_preprocessing(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "def fetch_docs_SUM_model(query_prepcsd):\n",
    "    doc_scores = defaultdict(lambda: 0)\n",
    "    \n",
    "    for query_term in query_prepcsd:\n",
    "        if query_term in inverted_index:\n",
    "            posting_list = inverted_index[query_term]\n",
    "            for doc_id in posting_list.keys():\n",
    "                doc_scores[doc_id] += posting_list[doc_id]\n",
    "            \n",
    "    \n",
    "    ranked_list = sorted(list(doc_scores.items()), key=itemgetter(1), reverse=True)\n",
    "    return ranked_list\n",
    "\n",
    "def fetch_docs_AND_model(query_prepcsd):\n",
    "    doc_scores = defaultdict(lambda: 0)\n",
    "    \n",
    "    and_docs = []\n",
    "    for query_term in query_prepcsd:\n",
    "        if query_term in inverted_index:\n",
    "            posting_list = inverted_index[query_term]\n",
    "            docs = posting_list.keys()\n",
    "            if len(and_docs) == 0:\n",
    "                and_docs = docs\n",
    "            else:\n",
    "                and_docs = list(set(and_docs) & set(docs))\n",
    "\n",
    "            for doc_id in posting_list.keys():\n",
    "                doc_scores[doc_id] += posting_list[doc_id]\n",
    "            \n",
    "    and_doc_scores = [(doc_id, doc_scores[doc_id]) for doc_id in and_docs]\n",
    "    ranked_list = sorted(and_doc_scores, key=itemgetter(1), reverse=True)\n",
    "    return ranked_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 0.49854387598474703)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_docs_AND_model(query_prepcsd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Reading\n",
    "1. https://nlp.stanford.edu/IR-book/information-retrieval-book.html\n",
    "2. https://www.elastic.co/guide/en/elasticsearch/reference/current/getting-started.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
